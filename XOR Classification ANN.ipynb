{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekpadghane/demo/blob/master/XOR%20Classification%20ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TH7no-hDNUCE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pylab\n",
        "fig = plt.figure()\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQ6d550k-Opj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# XOR Classification Problem\n",
        "In this, i will demostrate XOR gate problem, why **Single Layer Perceptron** can't clasify this kind of data and the solution we got using ** Multi Layer Perceptron**."
      ]
    },
    {
      "metadata": {
        "id": "IO0mhY8I_CWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SLP** are used to classify **Linearly Seperable data** like classifying labels using a **Line**. But XOR gate output is **not Linearly Separable**. Thats why we need to use a **MLP** which has atleast one **Hidden Layer** responsible of classifying **non linearly separable** data."
      ]
    },
    {
      "metadata": {
        "id": "A18Fl5sa_-Bk",
        "colab_type": "code",
        "outputId": "271946f8-b6f6-4045-d57a-64f8dc90f476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "# Creating Input Matrix X, Output vector y.\n",
        "# XOR gate output is 0,1,1,0 for combinations (0,0), (0,1), (1,0), (1,1)\n",
        "\n",
        "X = np.array([[0,0,1,1], [0,1,0,1]]).reshape(2,-1).T\n",
        "y = np.array([0,1,1,0]).reshape(-1,1)\n",
        "\n",
        "# Ploting the values\n",
        "\n",
        "plt.plot(X.T[0], X.T[1], 'o')\n",
        "plt.xlabel('A')\n",
        "plt.ylabel('B')\n",
        "plt.title('Plotting Outputs of XOR gate')\n",
        "fig.canvas.draw()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF6tJREFUeJzt3X+0XGV97/F35CjXQLge4oEQikRr\n+EpstU1aSER+SFyIjbetq6m1aisaqpb0Fui6cqloLxYt/SHNFeyVUFvptS26oBcMBSrrYlFqWBRD\nS23ALyImGE9uOISUxATFJOf+sfeBYTJnkpzMnsPJ836txWJm79l7f5+Zk+cz+3lmZk8bHR1FklSe\nF0x2AZKkyWEASFKhDABJKpQBIEmFMgAkqVAGgCQVamCyC9DzT0SMAt8GdlK9SXgSuDgz74iIM4DP\nZOYr97KPNwEPZuajEXE0cHJmroqIk4DLMvNNPaz3Z4GPAy8HRoFh4NLMvHMftn2mzgM4/m9k5p9P\ndPu2fZ0H/B5wVWZ+vGX5TwJ3AvMzc329bBbwDeDszFwTEdOA3wbOBV5I9dr9I/DhzBypt7kTOAHY\nWu96gOq1/q+Z+VCP2vDM692L/ak5ngFoPGdk5qsy8wTgAuD6iBjaj+0vBF5W334D8PMAmfnPPe78\nXwPcBnw6M+fW9f4+8IWIWLyfdU7k+LOAiya6fQe/BFzS2vkDZOY3gBXAypbFVwHXZOaa+v7HgXcC\nb87MVwHzgP8A7oyIF7dsd1H92r6qDvI7gM/2sA3PvN56fvMMQHuVmV+LiIeBRTz7zpGI+E/A/6T6\nB78buJWqM7wUWAycGBH/C/ggMBARhwNXU59BRMSlwEuBY4HXAo8Dv5CZGyNiPvCF+lB/TdUx/naH\nd/UfAa7OzBtb6v1yRFwGXAbcERHXAg9n5sfquq8FHgYObanzIuDNwBbgp6jeJa8B3p6ZO+qzouMy\nc0O9j1HgOOCrwI9FxDeB1wDvA5YD0+rn6j2Zuba14C7P2+X1c3xiRByXmZe2tfUPgXsi4pz6uZoH\nvKve55FUQf1TYzVm5k7gv9dB+GvANXT2ReB/dFpRnyF9Bvg+VQB9AnhNZq6LiI/Uxx8AHqxvvwL4\nFPXrnZlvj4hfAD4GHFY/7+/IzMfHqUV95BmA9tULgR+2LbuAqhN8NTAfOBX41cz8CPA94J2Z+UdU\nHcINmfn2Dvv95Xo/Pw48Bry3Xn4N8KeZOZdqCOqEceo6Hfj7DstvBk6qO9uO2uocC5u3Akvrdv1n\n4DfG2772XuDR+h33oVShc1J9/0+AJR22Ge95uwj4Z6p36Jd2qHcncA5VUHyKKlzGXpOFdR2dhnFu\npnqe9hARA8D7gdUd1h0C/BXwvsw8EZhL1YkTEQuA3wJ+tl5+KPBbmXkfLa93RLwC+FzdvldQDUld\n3akW9Z8BoL2KiDcDs4Cvta1aQjUEsTMznwL+BjhrP3f/1cxcn5mjwL8AL6uHKxYA19WP+TOqd9Sd\nHAmMdFi+CTiEqhPfH1/MzM2ZuRu4CXjdfmz7A6o5iGURcXRmXp+Zf9zhcRN+3uqhoHVU8zP3tawa\n73mA6rk4suX+H0fENyMige3AIPCODtudAByambfV96+i7jPqYafjMnNr/Vytpnr33+5s4M7M/Pf6\n/tXAz9fhoklmAGg8d9adxENU4+Rvzszvtz1miGrIZMwW4Kj9PM6TLbd3UXXag8BoZv4HQGb+iOrs\noJPHgdkdlh9N1Ulu6bCumydabm+pa9kndZ2LgVOAhyLirnrytt2En7eIeC/wFLCW5849jPc8QPVc\ntD5/Y3MAQTXM9U9jk8RtBtvqHG6pYzpwVURkHSTn0bk/eQlwWv239E3gbqrXfGaXZqpPnAPQeM4Y\nG0vuYhPP/Yc8s152oLYC0yJiej3+PkDVaXZyG9WwzV1ty/8LcFdmPh0RY8Eyplun/tKW20fybCDs\nHttHRIy7fWb+C/DLEfEiqg76aqpAaDWh5y0ifoxqovdUquG4+yLi7zIzqTrWIyPitZl5f9umb6F6\n997Jh4C/iYjrMnNH27qtwOEt92e13L6AauhnQWZ+PyI+TjWX024Y+L+ZuXRv7VP/eQagA/H3VMMd\nh0TEYVQTjbfU635E9e6v/fZe1WcaDwJvqxe9n2popZOPAr8eEc8MYUTE6VQd24frRRupJpmpx6Rf\n37J9e21nR8RL6iGKX+TZYHlmH1Tj/rtbtj88IgYi4icj4vqIeFFmPg18fZy6uz1v3fwFsCIzH87M\n71JNrH4mIqZl5pNU4fC5iHh53daBiLicKrg+32mH9aT6WqqJ+nbfAl5Yf/QX4AMt7TkK+Gbd+R8P\n/BzPhkXrc/ol4NT6eSciToqIT+5DW9UHBoAOxFXAd6k6kK9TdWzX1+tuAD4fEb8D3A6cGRH37se+\nzwMuiYi1VBOP36NDZ5qZ66jGz98dEQ/XQ1YfBd6WmWMTm38OzImIb1FNoN7QsovWOqH6SOT/ATZQ\nDX/8Zb38EuDTEfGvVOPmY5+G+jeqs4T/Vy/7DrC2rvtS4PwObev2vHUUEe+jOgv607b9HEr1qSMy\n8xNUk+c318MtD1CdxbyxDqTxfAj4b/VHWp9RTzD/JnBt3e6HqIJvlOrM5vR6+OcK4HeAxRFxAS2v\nd2ZupJpIvzEiHqSaIP4Cel6Y5vUA9HxVv7MdrW+PUHVk7cMbvTzetbR8XFTPVZ+tfB94SX3GoSnO\nMwA9L0XE9dSTnBFxJtWngHryTVXtu4i4NyJ+pb77K1TfmrbzP0g4Caznq98DPhsRy4CngV+rPzKp\n/roQ+LP6i3VbgXdPcj3qIYeAJKlQDgFJUqGmzBDQyMi2CZ+qDA5OZ8uW9o84H9xscxlscxkOpM1D\nQzPG+xZ9GWcAAwPlfevcNpfBNpehqTYXEQCSpD0ZAJJUKANAkgplAEhSoQwASSpUox8DjYifoLrc\n3IrM/FTbujcCf0D1G/C3ZuZlvT7+PQ9s4pa71zG8eQezZ05nyaI5nDzv6F4fRpIa0XQf1tgZQP3D\nUVdR/bpiJ1dSXef1FOCsiJjXy+Pf88AmVq5ay4aR7ezePcqGke2sXLWWex7oxc/VS1Kz+tGHNTkE\n9EOq3wgfbl9R/zb4E5n53fpycrdSXUmpZ265e904y9f38jCS1Ih+9GGNDQHVF7DeGRGdVs/iudcv\nfYzqouDjGhycvl9fhhje3Plbcxs3b2doaMY+72cqK6WdrWxzGUpocz/6sOfLT0GM+1XlMfv7NejZ\nM6ezYWT7HsuPmXkYIyPb9mtfU9HQ0Iwi2tnKNpehlDb3qg/rFhaT9SmgYZ57fdFj6TBUdCCWLJoz\nzvLje3kYSWpEP/qwSTkDyMx1EXFERMyhuvTeW4B39vIYYzPlt9y9no2bt3PMzMNYsuh4PwUkaUro\nRx/WWABExAKqa4XOAX4UEUuBVcB3MvNGqmuNXlc//AuZ2fOrPZ0872hOnnd0MaeMkg4uTfdhTU4C\nrwHO6LL+q8Cipo4vSerObwJLUqEMAEkqlAEgSYUyACSpUAaAJBXKAJCkQhkAklQoA0CSCmUASFKh\nDABJKpQBIEmFMgAkqVAGgCQVygCQpEIZAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoA\nkKRCGQCSVCgDQJIKZQBIUqEMAEkqlAEgSYUyACSpUAaAJBXKAJCkQhkAklSogSZ3HhErgIXAKHB+\nZt7bsm458C5gF/D1zLygyVokSc/V2BlARJwOzM3MRcAy4MqWdUcAHwROzczXA/MiYmFTtUiS9tTk\nENBi4CaAzHwQGKw7foCn6/8Oj4gBYDrwRIO1SJLaNDkENAtY03J/pF62NTN/EBEfBR4BngI+n5kP\nddvZ4OB0BgYOmXAxQ0MzJrztVGWby2Cby9BEmxudA2gzbexGfSbwIeAEYCvw5Yh4bWbeP97GW7bs\nmPCBh4ZmMDKybcLbT0W2uQy2uQwH0uZuwdHkENAw1Tv+MbOBjfXtE4FHMvPxzHwauAtY0GAtkqQ2\nTQbA7cBSgIiYDwxn5liErQNOjIgX1/d/BvhWg7VIkto0NgSUmasjYk1ErAZ2A8sj4hzgycy8MSL+\nBPjHiNgJrM7Mu5qqRZK0p0bnADLz4rZF97esWwmsbPL4kqTx+U1gSSqUASBJhTIAJKlQBoAkFcoA\nkKRCGQCSVCgDQJIKZQBIUqEMAEkqlAEgSYUyACSpUAaAJBXKAJCkQhkAklQoA0CSCmUASFKhDABJ\nKpQBIEmFMgAkqVAGgCQVygCQpEIZAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRC\nGQCSVKiBJnceESuAhcAocH5m3tuy7jjgOuBFwH2Z+YEma5EkPVdjZwARcTowNzMXAcuAK9secgVw\nRWaeBOyKiJc1VYskaU9NDgEtBm4CyMwHgcGIOAIgIl4AnAqsqtcvz8xHG6xFktSmySGgWcCalvsj\n9bKtwBCwDVgREfOBuzLzd7vtbHBwOgMDh0y4mKGhGRPedqqyzWWwzWVoos2NzgG0mdZ2+1jgk8A6\n4JaIWJKZt4y38ZYtOyZ84KGhGYyMbJvw9lORbS6DbS7DgbS5W3A0OQQ0TPWOf8xsYGN9+3FgfWZ+\nOzN3AXcAr26wFklSmyYD4HZgKUA9zDOcmdsAMnMn8EhEzK0fuwDIBmuRJLVpbAgoM1dHxJqIWA3s\nBpZHxDnAk5l5I3ABcG09IfwN4OamapEk7anROYDMvLht0f0t6x4GXt/k8SVJ4/ObwJJUKANAkgpl\nAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCSVCgDQJIKZQBIUqEMAEkqlAEgSYXa66+BRsRr\ngE2ZuSkizgPeBPw78LHMfKrpAiVJzeh6BhARlwM3AHdHxO8Ci4C/AA4FVjZfniSpKXs7AzgTeBXw\nUmAtcEx9Na9VEfG1pouTJDVnb3MA2zNzd2Y+BjxQd/5jnm6wLklSw/ZnEnhX2/3RXhYiSeqvvQ0B\nvS4iHq1vH9VyexrVsJAkaYraWwBEX6qQJPVd1wDIzPX9KkSS1F9+EUySCmUASFKhDABJKpQBIEmF\nMgAkqVAGgCQVygCQpEIZAJJUKANAkgplAEhSofZ6RbADERErgIVUvxx6fmbe2+ExlwOLMvOMJmuR\nJD1XY2cAEXE6MDczFwHLgCs7PGYecFpTNUiSxtfkENBi4CaAzHwQGIyII9oecwVwSYM1SJLG0eQQ\n0CxgTcv9kXrZVoCIOAf4CrBuX3Y2ODidgYFDJlzM0NCMCW87VdnmMtjmMjTR5kbnANpMG7sREUcC\n7wHeCBy7Lxtv2bJjwgceGprByMi2CW8/FdnmMtjmMhxIm7sFR5NDQMNU7/jHzAY21rfPBIaAu4Ab\ngfn1hLEkqU+aDIDbgaUAETEfGM7MbQCZeUNmzsvMhcBbgfsy88IGa5EktWksADJzNbAmIlZTfQJo\neUScExFvbeqYkqR91+gcQGZe3Lbo/g6PWQec0WQdkqQ9+U1gSSqUASBJhTIAJKlQBoAkFcoAkKRC\nGQCSVCgDQJIKZQBIUqEMAEkqlAEgSYUyACSpUAaAJBXKAJCkQhkAklQoA0CSCmUASFKhDABJKpQB\nIEmFMgAkqVAGgCQVygCQpEIZAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCS\nVKiBJnceESuAhcAocH5m3tuy7g3A5cAuIIFzM3N3k/VIkp7V2BlARJwOzM3MRcAy4Mq2h1wDLM3M\nU4AZwNlN1SJJ2lOTQ0CLgZsAMvNBYDAijmhZvyAzN9S3R4CZDdYiSWrT5BDQLGBNy/2RetlWgMzc\nChARxwBnAR/ptrPBwekMDBwy4WKGhmZMeNupyjaXwTaXoYk2NzoH0GZa+4KIOAq4GTgvMzd323jL\nlh0TPvDQ0AxGRrZNePupyDaXwTaX4UDa3C04mgyAYap3/GNmAxvH7tTDQbcBl2Tm7Q3WIUnqoMk5\ngNuBpQARMR8YzszWCLsCWJGZ/9BgDZKkcTR2BpCZqyNiTUSsBnYDyyPiHOBJ4EvArwNzI+LcepO/\nzcxrmqpHkvRcjc4BZObFbYvub7l9aJPHliR15zeBJalQBoAkFcoAkKRCGQCSVCgDQJIKZQBIUqEM\nAEkqlAEgSYUyACSpUAaAJBXKAJCkQhkAklQoA0CSCmUASFKhDABJKpQBIEmFMgAkqVAGgCQVygCQ\npEIZAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCSVCgDQJIKZQBIUqEMAEkq\n1ECTO4+IFcBCYBQ4PzPvbVn3RuAPgF3ArZl5Wa+Pf88Dm7jl7nUMb97B7JnTWbJoDifPO7rXh5Gk\nRjTdhzV2BhARpwNzM3MRsAy4su0hVwK/BJwCnBUR83p5/Hse2MTKVWvZMLKd3btH2TCynZWr1nLP\nA5t6eRhJakQ/+rAmh4AWAzcBZOaDwGBEHAEQEa8AnsjM72bmbuDW+vE9c8vd68ZZvr6Xh5GkRvSj\nD2tyCGgWsKbl/ki9bGv9/5GWdY8BP95tZ4OD0xkYOGSfDz68eUfH5Rs3b2doaMY+72cqK6WdrWxz\nGUpocz/6sEbnANpMm+A6ALZs6fxkjGf2zOlsGNm+x/JjZh7GyMi2/drXVDQ0NKOIdrayzWUopc29\n6sO6hUWTQ0DDVO/0x8wGNo6z7th6Wc8sWTRnnOXH9/IwktSIfvRhTZ4B3A58FFgZEfOB4czcBpCZ\n6yLiiIiYA2wA3gK8s5cHH5spv+Xu9WzcvJ1jZh7GkkXH+ykgSVNCP/qwaaOjoz3bWbuI+EPgNGA3\nsBz4aeDJzLwxIk4D/qh+6N9l5ie67WtkZNuECy3llLGVbS6DbS7DgbR5aGjGuEPsjc4BZObFbYvu\nb1n3VWBRk8eXJI3PbwJLUqEMAEkqlAEgSYUyACSpUI1+CkiS9PzlGYAkFcoAkKRCGQCSVCgDQJIK\nZQBIUqEMAEkqlAEgSYXq5wVh+mKyL0Q/GfbS5jcAl1O1OYFz68twTmnd2tzymMuBRZl5Rp/L67m9\nvMbHAdcBLwLuy8wPTE6VvbWXNi8H3kX1d/31zLxgcqrsvYj4CeCLwIrM/FTbup72YQfVGcBkX4h+\nMuxDm68BlmbmKcAM4Ow+l9hz+9Bm6tf2tH7X1oR9aO8VwBWZeRKwKyJe1u8ae61bm+tri38QODUz\nXw/Mi4iFk1Npb0XEYcBVwB3jPKSnfdhBFQBM8oXoJ8m4ba4tyMwN9e0RYGaf62vC3toMVad4Sb8L\na0i3v+sXAKcCq+r1yzPz0ckqtIe6vcZP1/8dHhEDwHTgiUmpsvd+CPwcHa6Q2EQfdrAFQPvF5scu\nRN9p3WPAMX2qq0nd2kxmbgWIiGOAs6j+aKa6rm2OiHOArwDr+lpVc7q1dwjYBqyIiH+qh70OBuO2\nOTN/QHW1wUeA9cA9mflQ3ytsQGbuzMynxlnd8z7sYAuAdgd0Ifopao92RcRRwM3AeZm5uf8lNe6Z\nNkfEkcB7qM4ADlbT2m4fC3wSOB346YhYMilVNav1NT4C+BBwAvBy4OSIeO1kFTaJDrgPO9gCYFIv\nRD9JurV57B/LbcCHM/P2PtfWlG5tPpPqXfFdwI3A/HoycSrr1t7HgfWZ+e3M3EU1dvzqPtfXhG5t\nPhF4JDMfz8ynqV7rBX2ubzL0vA872ALgdmApQKcL0QNHRMScetzwLfXjp7px21y7gurTBP8wGcU1\npNvrfENmzsvMhcBbqT4Vc+HkldoT3dq7E3gkIubWj11A9Wmvqa7b3/U64MSIeHF9/2eAb/W9wj5r\nog876H4OupcXop8qxmsz8CVgC3B3y8P/NjOv6XuRPdbtdW55zBzg2oPkY6Dd/q5fCVxL9YbuG8Bv\nHiQf9e3W5vdTDfXtBFZn5kWTV2nvRMQCqjdtc4AfAd+jmuD/ThN92EEXAJKkfXOwDQFJkvaRASBJ\nhTIAJKlQBoAkFcoAkKRCGQDSAYiIYyJiZ0RcPNm1SPvLAJAOzLuBB4BzJrkOab8ZANKBeS9wIXBY\nRLxusouR9ocBIE1Q/a3MAeDLwP+m+maqNGUYANLELaP6qYlR4LPA2yJi+iTXJO0zfwpCmoD6V1aH\ngUepLk4C8Eqq3+H53KQVJu2Hg+6awFKf/Crwlcx85rf3I+IdwLmAAaApwSEgaWKWAZ9uW3YD1fVp\n5/S/HGn/OQQkSYXyDECSCmUASFKhDABJKpQBIEmFMgAkqVAGgCQVygCQpEL9f/XwJz/lPE1VAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Pu4V7dHaBBuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating Weights, we have two sets of Weights because we have one Hidden layer\n",
        "# Coz we have 2 inputs units on our Input layer and 2 hidden units on our Hidden layer,\n",
        "# so, our weights for hidden layer is 2x2 = 4\n",
        "# Note, Weights are Random Initialization, in my case iam picking random samples from Uniform Distribution,\n",
        "# its not neccesary to pick from below distribution, any random value is applicable\n",
        "\n",
        "Weights_HL = np.random.uniform(size=(2,2))\n",
        "\n",
        "# As i mention we have one hidden layer, it means two sets of Weights,\n",
        "# Weights for hidden layer is done, now i will create weights for output layer\n",
        "# we have 1 output but 2 hidden units in our hidden layer, thats why output layer weights are 2x1 = 2\n",
        "\n",
        "Weights_OL = np.random.uniform(size=(2,1))\n",
        "\n",
        "# Actual function for each neuron is W.TX + b in which b is bias (intercept)\n",
        "# so we need to create bias\n",
        "# first i will create bias for hidden layer\n",
        "# number of bias depends on output to the another layer\n",
        "# for hidden layer, we have 2 hidden layer units, so bias for hidden layer is 2\n",
        "# Note, most of the cases bias is 1, but it can be any random assingnment\n",
        "\n",
        "Bias_HL = np.random.uniform(size=(2,1))\n",
        "\n",
        "# for output layer, we have 1 output, so bias is 1\n",
        "\n",
        "Bias_OL = np.random.uniform(size=(1,1))\n",
        "\n",
        "# creating hidden layer. we have 1 hideen layer with 2 hidden units,\n",
        "# therefore size of hidden layer is 2x1 = 2\n",
        "# Note, hiden layer stores activation(W.TX + b), for initializaion, assing with any number\n",
        "\n",
        "Hidden_Layer = np.random.uniform(size=(2,1))\n",
        "\n",
        "# creating output layer. we have 1 output unit (y predicted/estimated) in output layer,\n",
        "# therefore size of output layer is 1\n",
        "# Note, output layer stores activation(W.TH + b), for initializaion, assing with any number\n",
        "\n",
        "Output_Layer = np.random.uniform(size=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOT0vB_9SEkE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feedforward Step\n",
        "In this **Activated weighted sum** which is output of a layer is passed as input to the another layer. For first hidden layer, inputs are actual rows X corresponding to there y.\n",
        "\n",
        "##Activation Funtion\n",
        "Activation function is used to generate value for neuron which helps to classify the data. If Activation function is not given, then the model becomes **Regression model** in which relationship between X and y is figured, specially in the case of **continues data**.\n",
        "\n",
        "Here we have classification problem, thats why we need a activation function. In this case, i am using **Sigmoid function** which penalize the input given to it into a probability ranging from 0 to 1."
      ]
    },
    {
      "metadata": {
        "id": "oSxf2WF6T143",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating Activation function which in my case i am using is sigmoid, its defined as (1 + e ** (-x)) ** -1\n",
        "# Derivative of Sigmoid is sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def Sigmoid(x, diff=0):\n",
        "  if diff == 1:\n",
        "    return np.multiply(Sigmoid(x), np.subtract(1, Sigmoid(x)))\n",
        "  \n",
        "  return np.divide(1, np.add(1, np.exp(-x)))\n",
        "\n",
        "# Note, output of Sigmoid is between 0 and 1\n",
        "# Note, any other activation function is also applicable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CRtTd6RUU_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating feedforward step for neural network\n",
        "# functionality of this step is alredy discussed above in hidden and output layer\n",
        "\n",
        "def Feedforward(X, W_HL, W_OL, B_HL, B_OL):\n",
        "  HL = Sigmoid(W_HL.T.dot(X) + B_HL)\n",
        "  OL = Sigmoid(W_OL.T.dot(HL) + B_OL)\n",
        "  return HL, OL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZDWOAnOYl4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Backpropagation step\n",
        "\n",
        "In backpropagation, the weights of our network are adjusted while **minimizing the loss** of the model. Loss is error noted as difference between actual and predicted/estimated.\n",
        "\n",
        "The algorithm used for adjusting the weights is **Gradient descent** in which we derive the derivative because we need slope of loss function to go the loss where loss is closer to zero.\n",
        "\n",
        "The amount of iteration it requires for reaching to minimum loss is called **Traing iterations** or **epochs**.\n",
        "\n",
        "The amount of step or fraction which is used to approximate the minimum loss is called **Learning rate** or **step size** dinoted by alpha or eita. "
      ]
    },
    {
      "metadata": {
        "id": "Xapi0nLoXjIr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating backpropagation step for neural network\n",
        "# we will first update weights and biases of output layer then of the hidden layer\n",
        "# because its in reverse fashion, therefore updation of output layer weights and bias comes first\n",
        "\n",
        "def Backpropagation(X, y, W_HL, W_OL, B_HL, B_OL, HL, OL, alpha, epochs):\n",
        "  \n",
        "  # Error is Original minus predicted, in our model output layer has predicted value\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    \n",
        "    # First we need results of hidden and output layer, therefore we are using feedforward\n",
        "    \n",
        "    HL, OL = Feedforward(X, W_HL, W_OL, B_HL, B_OL)\n",
        "    \n",
        "    # Calculating error, delta for Output layer weights and updating output layer bias\n",
        "    \n",
        "    Error = y - OL\n",
        "    Delta_W_OL = np.multiply(Error, Sigmoid(OL, diff=1))\n",
        "    B_OL += np.multiply(Error, alpha)\n",
        "    \n",
        "    # Calculating error, delta for Hidden layer weights and updating hidden layer bias\n",
        "    \n",
        "    Error = Delta_W_OL.dot(W_OL.T)\n",
        "    Delta_W_HL = np.multiply(Error.T, Sigmoid(HL, diff=1))\n",
        "    B_HL += np.multiply(Error.T, alpha)\n",
        "    \n",
        "    # Ealuating dot product with the inputs to the hidden and output layer\n",
        "    \n",
        "    Adjust_W_HL = X.dot(Delta_W_HL.T)\n",
        "    Adjust_W_OL = HL.dot(Delta_W_OL)\n",
        "    \n",
        "    # Updating both the weights\n",
        "  \n",
        "    W_HL += np.multiply(Adjust_W_HL, alpha)\n",
        "    W_OL += np.multiply(Adjust_W_OL, alpha)\n",
        "    \n",
        "  return W_HL, W_OL, B_HL, B_OL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlOQMmy_C1U0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "61f1d6b9-ed20-4498-d995-8dad6e2ae6eb"
      },
      "cell_type": "code",
      "source": [
        "# Passing the dataset and checking the predictions\n",
        "# Note, alpha and epochs are hyperparameters which are selected by hit and trial \n",
        "\n",
        "alpha = 0.001\n",
        "epochs = 7000\n",
        "\n",
        "# For every X and y , we will train our neural network upto epochs \n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "  \n",
        "  # Extracting X and y according to index (eg. if i equals 1 then X is [0,1] and y is [1])\n",
        "  \n",
        "  X_ = X[i].reshape(-1,1)\n",
        "  y_ = y[i].reshape(-1,1)\n",
        "  \n",
        "  # Calling Backpropagation for training the model\n",
        "  # it will return weights and biases adjusted according to epochs and the decresed loss\n",
        "  \n",
        "  Weights_HL, Weights_OL, Bias_HL, Bias_OL, = Backpropagation(X_, y_, Weights_HL, Weights_OL, Bias_HL, Bias_OL, Hidden_Layer, Output_Layer, alpha, epochs)\n",
        "  \n",
        "  # to get predicted value, we need forward pass, or activated result\n",
        "  # for that forward pass is called which will provide predicted value according to the addjusted weights and biases\n",
        "  \n",
        "  HL, OL = Feedforward(X_, Weights_HL, Weights_OL, Bias_HL, Bias_OL)\n",
        "  \n",
        "  # printing the result\n",
        "  \n",
        "  print('X :', X_[0][0],' ',X_[1][0], ' y ', y_[0][0], ' y predicted ',OL[0][0])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X : 0   0  y  0  y predicted  0.008817584715394561\n",
            "X : 0   1  y  1  y predicted  0.7385322014849243\n",
            "X : 1   0  y  1  y predicted  0.9057911465483289\n",
            "X : 1   1  y  0  y predicted  0.17250082215477497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mGp-EU8pezN5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "As we can see, training upto 7000 iteration, our model is giving good results. Remember, if iteration are increased, we will get result closest to actual target value (original y)."
      ]
    },
    {
      "metadata": {
        "id": "8usfMa6NfDgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}